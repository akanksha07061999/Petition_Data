{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d69db6",
   "metadata": {},
   "source": [
    "# Transforming Government Petition Data\n",
    "\n",
    "## Objective\n",
    "We are provided with a JSON data file which contains some government petitions where we are given title of the petition labelled as *label*, main text of the petition labelled as *abstract* and number of signatures labelled as *numberOfSignatures*.\n",
    "\n",
    "In this report, our main objective is to to transform government petition data from a JSON format into a structured Pandas DataFrame. So, we have to create a CSV file with one row per petition with $21$ columns. These $21$ columns should include a new column called *petition_id* which needs to be created and one column for each of the $20$ words that appear the most number of times across all the abstracts of all petitions. The column *petition_id* will be a unique identifier to identify numerous petitions and the $20$ most common words must contain $5$ or more letters. These $20$ columns will store the count of each word for each\n",
    "petition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de7a291",
   "metadata": {},
   "source": [
    "## Code Overview\n",
    "First, we have started by importing all the necessary libraries which will be required for achieving the objective of this task. We have included $json$ for JSON parsing, $pandas$ for data manipulation, $Counter$ for word counting, and $nltk$ for natural language processing tasks. We have also downloaded essential resources from the Natural Language Toolkit (NLTK), such as tokenizers and stopwords, which are crucial for text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1979785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Akanksha99\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Akanksha99\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d937076d",
   "metadata": {},
   "source": [
    "When we want to extract the common words, there are some words which frequently occur in the English Language like “himself”, “the”, “is”, “are” etc. So, there is a list called stopwords in NLTK which we can use to filter such words. There are still some words which are not there in this list. So, we can customise this list and add the words which we don't need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee91efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get English stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Add 'could' to the stopwords list\n",
    "custom_stopwords = english_stopwords.copy()\n",
    "custom_stopwords.add('could')\n",
    "custom_stopwords.add('without')\n",
    "custom_stopwords.add('every')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97261e",
   "metadata": {},
   "source": [
    "This part of the code loads the input data from the specified JSON file, and then checks if there are any missing values in the objects, label and abstract in JSON file.\n",
    "\n",
    "To check if there are any missing values, we have created an empty list called missing_labels. Then, we will iterate through the json file and start petition_id from 1. This loop will check if there are any missing values in the object, abstract. If value is missing then the loop will put the id of that abstract in missing_labels list. Lastly, if there are any missing values, then the code will print the ids of those petitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c53f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data from the JSON file\n",
    "with open(\"input_data.json\", \"r\") as file:\n",
    "    input_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "555c0ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing abstracts found.\n"
     ]
    }
   ],
   "source": [
    "missing_labels = []\n",
    "for petition_id, petition_data in enumerate(input_data, start=1):\n",
    "    abstract = petition_data.get(\"abstract\", {}).get(\"_value\")\n",
    "    if abstract is None or not abstract.strip():\n",
    "        missing_labels.append(petition_id)\n",
    "\n",
    "if missing_labels:\n",
    "    print(\"Missing abstracts in petitions with indices: {missing_labels}\")\n",
    "else:\n",
    "    print(\"No missing abstracts found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb466896",
   "metadata": {},
   "source": [
    "Now, we know there are no missing values in the abstract object in json file. We have also similarly checked for the label object and found no missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a6cce",
   "metadata": {},
   "source": [
    "Then, we have created data pre-processing pipeline by creating the *preprocess_text()* function which will convert all the text to lowercase, remove punctuations, and retain only alphanumeric characters and whitespaces. This step will help us to extract words from sentences in an efficient way by ensuring a consistent format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "875180a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase and remove punctuation\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1442547",
   "metadata": {},
   "source": [
    "Now, we will create the function *get_top_words()* which will help us extract the most common words from input data. This function tokenizes the input texts i.e. extract each word from all the sentences, filters out stopwords using our customised list custom_stopwords and short words with length less than 5 letters. \n",
    "\n",
    "This is done by using a for loop which iterates through input and tokenizes them using *word_tokenize()* function. This means extracting each word from the input and put in a list called words. Then, a sub-loop iterates through this list and filters stop words and words with less than 5 letters from this list. \n",
    "\n",
    "This function also counts the occurrences of each words using counter from collections. Counter() creates an empty Counter object. A Counter is a collection where elements are stored as dictionary keys, and their counts as dictionary values.\n",
    "\n",
    "The top 20 words are selected based on their frequency using *most_common()* method of the Counter class. It returns a list of tuples where each tuple contains a word and its count. Lastly, a list called *top_words* is returned containing 20 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78cd7e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(texts, n=20):\n",
    "    # Count word occurrences in the texts\n",
    "    word_counts = Counter()\n",
    "    for text in texts:\n",
    "        words = word_tokenize(text)\n",
    "        for word in words:\n",
    "            if len(word) >= 5 and word not in custom_stopwords:\n",
    "                word_counts[word] += 1\n",
    "    \n",
    "    # Get the top n words\n",
    "    top_words = [word for word, _ in word_counts.most_common(n)]\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fcd7c",
   "metadata": {},
   "source": [
    "Then, we have created a second function called, *create_petition_df()* which is the core of the code, responsible for transforming the input petition data into a structured DataFrame. It returns a Pandas DataFrame with 21 columns as required in the task. \n",
    "\n",
    "In this, we have created a loop which iterates through the input data, creates a list of all abstracts by extracting the \"_value\" field from the \"abstract\" key for each petition in the input data and puts it into variable, abstracts. Then, we pre-process all the abstracts using *preprocess_text()* and put it in the list called preprocessed_abstracts. Finally, we get the top 20 common words using *get_top_words()* function.\n",
    "\n",
    "Now after this, we create a loop which iterates through the input data and starts the petition_id from 1. The loop extracts abstract from each petition, pre-process it and then appends it to the petitions list for each petition, including \"petition_id\" and the count of each top word in the preprocessed abstract. Finally, we convert this to a Pandas Dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3484409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_petition_df(data):\n",
    "    petitions = []\n",
    "    \n",
    "    # Extract the abstracts and petition IDs\n",
    "    abstracts = [petition[\"abstract\"][\"_value\"] for petition in data]\n",
    "    # Preprocess abstracts\n",
    "    preprocessed_abstracts = [preprocess_text(abstract) for abstract in abstracts]\n",
    "\n",
    "    # Get the top 20 words\n",
    "    top_words = get_top_words(preprocessed_abstracts, n=20)\n",
    "\n",
    "    # Create DataFrame\n",
    "    for petition_id, petition_data in enumerate(data, start=1):\n",
    "        abstract = petition_data[\"abstract\"][\"_value\"]\n",
    "\n",
    "        # Preprocess abstract\n",
    "        processed_abstract = preprocess_text(abstract)\n",
    "\n",
    "        petitions.append({\n",
    "            \"petition_id\": petition_id,\n",
    "            **{word: processed_abstract.count(word) for word in top_words}\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(petitions)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12932301",
   "metadata": {},
   "source": [
    "The below code calls the *create_petition_df()* to create the petition DataFrame, and then saves the DataFrame to a CSV file named \"output.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac194d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with the desired structure\n",
    "petition_df = create_petition_df(input_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "petition_df.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8441302",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The provided code successfully achieves its objective of transforming government petition data into a structured CSV file, incorporating natural language processing techniques to identify and count the most common words. \n",
    "\n",
    "Now, the dataframe petition_df can be utilized for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbe60a7",
   "metadata": {},
   "source": [
    "## Test\n",
    "Now lets us perform a set of unit tests using the \"unittest\" framework to test functions related to transforming government petition data.\n",
    "\n",
    "We have first imported \"unittest\" module, which provides a testing framework for writing and running tests in Python. Then, we define a test class named \"TestPetitionTransformation\" that inherits from unittest.TestCase. Each method in this class represents an individual test case.\n",
    "\n",
    "*preprocess_text()* function checks if the function correctly converts the input text to lowercase and removes punctuation. The assertion is that the processed text should be \"hello world\".\n",
    "\n",
    "*get_top_words()* function provides a set of texts and checks if the function correctly extracts the top 2 words with a minimum length of 5 characters. The assertion is that the extracted top words should be \"sauce\" and \"tasty\".\n",
    "\n",
    "*create_petition_df()* function provides a set of input data and checks if the function correctly creates a DataFrame. The assertion is that the columns of the DataFrame should include the expected top words (\"sauce\" and \"tasty\").\n",
    "\n",
    "Then we run all the test methods in the TestPetitionTransformation class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53355454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.004s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit tests\n",
    "import unittest\n",
    "\n",
    "class TestPetitionTransformation(unittest.TestCase):\n",
    "    def test_preprocess_text(self):\n",
    "        self.assertEqual(preprocess_text(\"Hello, World!\"), \"hello world\")\n",
    "\n",
    "    def test_get_top_words(self):\n",
    "        texts = [\"This is a tasty sauce.\", \"sauce is very tasty.\", \"I like tomato sauce.\"]\n",
    "        self.assertCountEqual(get_top_words(texts, n=2), [\"sauce\", \"tasty\"])\n",
    "\n",
    "    def test_create_petition_df(self):\n",
    "        input_data = [\n",
    "            {\"abstract\": {\"_value\": \"This is a tasty sauce.\"}},\n",
    "            {\"abstract\": {\"_value\": \"sauce is very tasty.\"}}\n",
    "        ]\n",
    "\n",
    "        df = create_petition_df(input_data)\n",
    "        expected_top_words = set(get_top_words([\"This is a tasty sauce.\", \"sauce is very tasty.\"], n=2))\n",
    "\n",
    "        self.assertTrue(expected_top_words.issubset(set(df.columns)))\n",
    "\n",
    "# Run the tests\n",
    "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestPetitionTransformation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3b589c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
